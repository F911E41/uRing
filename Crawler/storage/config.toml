# Crawler Configuration

[crawler]
# User agent string for HTTP requests
user_agent = "Mozilla/5.0 (compatible; uRing Crawler/0.1)"

# Request timeout in seconds
timeout_secs = 30

# Delay between requests in milliseconds (to be polite to servers)
request_delay_ms = 100

# Timeout for sitemap/secondary requests
sitemap_timeout_secs = 5

# Maximum concurrent requests (0 = sequential)
max_concurrent = 5

[paths]
# Seed configuration file (campuses and keywords)
seed = "data/seed.toml"

# Output directory
output_dir = "storage"

# Output file names
manual_review_file = "Temp/manual_review_needed.json"
departments_file = "Temp/yonsei_departments.json"
departments_boards_file = "siteMap.json"

[cleaning]
# Patterns to remove from titles
title_remove_patterns = ["첨부파일", "공지"]

# Patterns to remove from dates
date_remove_patterns = ["작성일"]

# Date replacement patterns (from -> to)
[[cleaning.date_replacements]]
from = ". "
to = "."

[output]
# Enable console output
console_enabled = false

[logging]
# Log level: error, warn, info, debug, trace
level = "info"

# Show progress indicators
show_progress = true

[discovery]
# Maximum text length for board link names (longer = likely article title)
max_board_name_length = 20

# Patterns that indicate an article view (not a board listing)
blacklist_patterns = [
    "articleNo",
    "article_no",
    "mode=view",
    "seq",
    "view.do",
    "board_seq",
]
